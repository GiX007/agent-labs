{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GiX007/agent-labs/blob/main/03_langChain/00_template_prompts_and_output_parsers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "073bf8f9",
      "metadata": {
        "id": "073bf8f9"
      },
      "source": [
        "# LangChain: Models, Prompts and Output Parsers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Outline"
      ],
      "metadata": {
        "id": "UENUCF0MTL4H"
      },
      "id": "UENUCF0MTL4H"
    },
    {
      "cell_type": "markdown",
      "source": [
        " * Direct API calls to OpenAI\n",
        " * API calls through LangChain:\n",
        "   * Prompts\n",
        "   * Models\n",
        "   * Output parsers"
      ],
      "metadata": {
        "id": "pzBKpRDdTNrh"
      },
      "id": "pzBKpRDdTNrh"
    },
    {
      "cell_type": "markdown",
      "id": "a01ff606",
      "metadata": {
        "id": "a01ff606"
      },
      "source": [
        "## Get your [OpenAI API Key](https://platform.openai.com/account/api-keys)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70aa2619",
      "metadata": {
        "height": 47,
        "tags": [],
        "id": "70aa2619"
      },
      "outputs": [],
      "source": [
        "#!pip install python-dotenv\n",
        "#!pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7ed03ed-1322-49e3-b2a2-33e94fb592ef",
      "metadata": {
        "height": 115,
        "tags": [],
        "id": "b7ed03ed-1322-49e3-b2a2-33e94fb592ef"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "dotenv_path = find_dotenv() or '/content/OPENAI_API_KEY.env' # read local .env file\n",
        "load_dotenv(dotenv_path)\n",
        "\n",
        "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
        "client = openai.OpenAI(api_key=openai_api_key)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "719a92fb-8227-4513-8950-c965b822c425",
      "metadata": {
        "id": "719a92fb-8227-4513-8950-c965b822c425"
      },
      "source": [
        "Note: LLM's do not always produce the same results. When executing the code in your notebook, you may get slightly different answers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4336d784-65c2-4a11-8489-b445b1fad177",
      "metadata": {
        "height": 249,
        "id": "4336d784-65c2-4a11-8489-b445b1fad177"
      },
      "outputs": [],
      "source": [
        "# Set the model variable based on the best and cheapest available choice at the current date\n",
        "llm_model = \"gpt-4o-mini\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bbad9cdb",
      "metadata": {
        "id": "bbad9cdb"
      },
      "source": [
        "## Chat API : OpenAI"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start with a direct API calls to OpenAI."
      ],
      "metadata": {
        "id": "m2TNGkGWUE4D"
      },
      "id": "m2TNGkGWUE4D"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "484bfa6a",
      "metadata": {
        "height": 166,
        "tags": [],
        "id": "484bfa6a"
      },
      "outputs": [],
      "source": [
        "def get_completion(prompt, model=llm_model):\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=0,\n",
        "    )\n",
        "    return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1d076ce",
      "metadata": {
        "height": 30,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "a1d076ce",
        "outputId": "8c91cd9e-6667-4cbd-9f10-125d70d96aab"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1 + 1 equals 2.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "get_completion(\"What is 1+1?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b32b57a",
      "metadata": {
        "height": 149,
        "tags": [],
        "id": "1b32b57a"
      },
      "outputs": [],
      "source": [
        "customer_email = \"\"\"\n",
        "Arrr, I be fuming that me blender lid flew off and splattered me kitchen walls with smoothie! And to make matters worse, the warranty don't cover the cost of cleaning up me kitchen. I need yer help right now, matey!\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18c34459",
      "metadata": {
        "height": 64,
        "tags": [],
        "id": "18c34459"
      },
      "outputs": [],
      "source": [
        "style = \"\"\"American English in a calm and respectful tone\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80b558e2",
      "metadata": {
        "height": 132,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80b558e2",
        "outputId": "6a67bb60-2d00-4ac5-d7ac-bf4bd911c6d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translate the text that is delimited by triple backticks into a style that is American English in a calm and respectful tone. \n",
            "text: ```\n",
            "Arrr, I be fuming that me blender lid flew off and splattered me kitchen walls with smoothie! And to make matters worse, the warranty don't cover the cost of cleaning up me kitchen. I need yer help right now, matey!\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "prompt = f\"\"\"Translate the text that is delimited by triple backticks into a style that is {style}.\n",
        "text: ```{customer_email}```\"\"\"\n",
        "\n",
        "print(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c883dcbd",
      "metadata": {
        "height": 30,
        "tags": [],
        "id": "c883dcbd"
      },
      "outputs": [],
      "source": [
        "response = get_completion(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99b33f61",
      "metadata": {
        "height": 30,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "99b33f61",
        "outputId": "e460b8f6-7f62-4cd6-a2cd-54e3f537ef6b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I’m quite frustrated that the lid of my blender came off and splattered my kitchen walls with smoothie. To make matters worse, the warranty doesn’t cover the cost of cleaning up my kitchen. I would really appreciate your assistance with this issue. Thank you!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "response"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f80482d1",
      "metadata": {
        "id": "f80482d1"
      },
      "source": [
        "## Chat API : LangChain"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try how we can do the same using LangChain."
      ],
      "metadata": {
        "id": "OacLc8MWVeWn"
      },
      "id": "OacLc8MWVeWn"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain-openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGqkXA2AkrD2",
        "outputId": "98b015a9-0844-4a49-d4a8-a616bced6162"
      },
      "id": "pGqkXA2AkrD2",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (0.3.27)\n",
            "Requirement already satisfied: langchain-openai in /usr/local/lib/python3.12/dist-packages (0.3.35)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.79)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.11)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.4.41)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.12.4)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.0.44)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.32.5)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain) (6.0.3)\n",
            "Requirement already satisfied: openai<3.0.0,>=1.104.2 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (1.109.1)\n",
            "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (0.12.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (4.15.0)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (25.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (3.11.4)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.25.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.104.2->langchain-openai) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.104.2->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.104.2->langchain-openai) (0.11.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.104.2->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.104.2->langchain-openai) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.5 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.41.5)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (2025.10.5)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.4)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai) (2024.11.6)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import langchain\n",
        "print(\"LangChain version:\", langchain.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "csgP3T4MlWzC",
        "outputId": "ec554d41-53c6-4d26-b67a-326b30161191"
      },
      "id": "csgP3T4MlWzC",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LangChain version: 0.3.27\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a25c5b27",
      "metadata": {
        "id": "a25c5b27"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0d4a269",
      "metadata": {
        "height": 30,
        "tags": [],
        "id": "f0d4a269"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1cc0c8b8",
      "metadata": {
        "height": 81,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1cc0c8b8",
        "outputId": "91350443-70e2-428d-bb18-30711238b9c0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7e813f89e930>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7e813f4e3bc0>, root_client=<openai.OpenAI object at 0x7e813fba04a0>, root_async_client=<openai.AsyncOpenAI object at 0x7e813f89e9f0>, model_name='gpt-4o-mini', temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********'), stream_usage=True)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# To control the randomness and creativity of the generated text by an LLM, use temperature = 0.0\n",
        "chat = ChatOpenAI(temperature=0.0, model=llm_model)\n",
        "chat"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4d07256",
      "metadata": {
        "id": "b4d07256"
      },
      "source": [
        "### Prompt template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57bda7d8",
      "metadata": {
        "height": 98,
        "tags": [],
        "id": "57bda7d8"
      },
      "outputs": [],
      "source": [
        "template_string = \"\"\"Translate the text that is delimited by triple backticks into a style that is {style}.\n",
        "text: ```{text}```\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a31f246",
      "metadata": {
        "height": 81,
        "tags": [],
        "id": "3a31f246"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "# create the first template\n",
        "prompt_template = ChatPromptTemplate.from_template(template_string)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2hp3Dp8XZ6Z",
        "outputId": "99acf146-4417-4532-9baa-01aa69d346de"
      },
      "id": "d2hp3Dp8XZ6Z",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptTemplate(input_variables=['style', 'text'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['style', 'text'], input_types={}, partial_variables={}, template='Translate the text that is delimited by triple backticks into a style that is {style}.\\ntext: ```{text}```\\n'), additional_kwargs={})])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cac2cb16",
      "metadata": {
        "height": 30,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cac2cb16",
        "outputId": "5fe3c504-8571-42a0-ac2c-2aae059b06b4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PromptTemplate(input_variables=['style', 'text'], input_types={}, partial_variables={}, template='Translate the text that is delimited by triple backticks into a style that is {style}.\\ntext: ```{text}```\\n')"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "prompt_template.messages[0].prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cdc5566c",
      "metadata": {
        "height": 30,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cdc5566c",
        "outputId": "c5e8254a-0141-4bdd-f8f3-9f58659a0cb2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['style', 'text']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "prompt_template.messages[0].prompt.input_variables"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Having created a prompt template  with 2 input variables, let's try it for a simple example.\n",
        "\n",
        "Example 1."
      ],
      "metadata": {
        "id": "bdDWXy8NYGd1"
      },
      "id": "bdDWXy8NYGd1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48989d11",
      "metadata": {
        "height": 149,
        "tags": [],
        "id": "48989d11"
      },
      "outputs": [],
      "source": [
        "customer_email = \"\"\"\n",
        "Arrr, I be fuming that me blender lid flew off and splattered me kitchen walls with smoothie! And to make matters worse, the warranty don't cover the cost of cleaning up me kitchen. I need yer help right now, matey!\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bbd51a93",
      "metadata": {
        "height": 64,
        "tags": [],
        "id": "bbd51a93"
      },
      "outputs": [],
      "source": [
        "customer_style = \"\"\"American English in a calm and respectful tone\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dff3954f",
      "metadata": {
        "height": 64,
        "tags": [],
        "id": "dff3954f"
      },
      "outputs": [],
      "source": [
        "# notice how we define the new message by using 'format_messages' method of the template\n",
        "customer_messages = prompt_template.format_messages(\n",
        "                    style=customer_style,\n",
        "                    text=customer_email)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c09d8b4",
      "metadata": {
        "height": 47,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8c09d8b4",
        "outputId": "08caade8-365b-42fb-ab86-53e30250eba1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n",
            "<class 'langchain_core.messages.human.HumanMessage'>\n"
          ]
        }
      ],
      "source": [
        "print(type(customer_messages))\n",
        "print(type(customer_messages[0]))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(customer_messages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vgCdrOT6Y-b0",
        "outputId": "6c04113d-af9d-4ad2-af1b-807f6d4947e6"
      },
      "id": "vgCdrOT6Y-b0",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[HumanMessage(content=\"Translate the text that is delimited by triple backticks into a style that is American English in a calm and respectful tone.\\ntext: ```\\nArrr, I be fuming that me blender lid flew off and splattered me kitchen walls with smoothie! And to make matters worse, the warranty don't cover the cost of cleaning up me kitchen. I need yer help right now, matey!\\n```\\n\", additional_kwargs={}, response_metadata={})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e02dafa2",
      "metadata": {
        "height": 30,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e02dafa2",
        "outputId": "a4f46398-a1d9-4c0e-b586-05aa797e08c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content=\"Translate the text that is delimited by triple backticks into a style that is American English in a calm and respectful tone.\\ntext: ```\\nArrr, I be fuming that me blender lid flew off and splattered me kitchen walls with smoothie! And to make matters worse, the warranty don't cover the cost of cleaning up me kitchen. I need yer help right now, matey!\\n```\\n\" additional_kwargs={} response_metadata={}\n"
          ]
        }
      ],
      "source": [
        "print(customer_messages[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd789f9f",
      "metadata": {
        "height": 47,
        "tags": [],
        "id": "bd789f9f"
      },
      "outputs": [],
      "source": [
        "# Call the LLM to translate to the style of the customer message\n",
        "customer_response = chat.invoke(customer_messages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad294407",
      "metadata": {
        "height": 30,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ad294407",
        "outputId": "0cd505e6-4752-4a4f-8fc1-7cf46ddfb55f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I am quite frustrated that the lid of my blender came off and splattered smoothie all over my kitchen walls. To make matters worse, the warranty does not cover the cost of cleaning up my kitchen. I would appreciate your assistance with this issue. Thank you.\n"
          ]
        }
      ],
      "source": [
        "print(customer_response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Translate Text into a Friendly Customer Tone (LangChain + GPT-4o-mini) - The simplest workflow\n",
        "\n",
        "```python\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "# 1. Initialize model\n",
        "chat = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7)\n",
        "\n",
        "# 2. Define prompt\n",
        "prompt = ChatPromptTemplate.from_template(\n",
        "    \"Translate this message into a friendly customer tone: {message}\"\n",
        ")\n",
        "\n",
        "# 3. Format input\n",
        "customer_messages = [prompt.format_messages(message=\"I need a refund right now!\")]\n",
        "\n",
        "# 4. Invoke model\n",
        "response = chat.invoke(customer_messages[0])\n",
        "\n",
        "# 5. Print output\n",
        "print(response.content)"
      ],
      "metadata": {
        "id": "q3BnGxf4aSV1"
      },
      "id": "q3BnGxf4aSV1"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example 2."
      ],
      "metadata": {
        "id": "1npU3sombyl8"
      },
      "id": "1npU3sombyl8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c267e5f",
      "metadata": {
        "height": 166,
        "tags": [],
        "id": "7c267e5f"
      },
      "outputs": [],
      "source": [
        "service_reply = \"\"\"Hey there customer, the warranty does not cover cleaning expenses for your kitchen because it's your fault that you misused your blender by forgetting to put the lid on before starting the blender. Tough luck! See ya!\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ff72bd1",
      "metadata": {
        "height": 81,
        "tags": [],
        "id": "2ff72bd1"
      },
      "outputs": [],
      "source": [
        "service_style_pirate = \"\"\"a polite tone that speaks in English Pirate\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d9e8f3f",
      "metadata": {
        "height": 98,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7d9e8f3f",
        "outputId": "ee78151c-c61e-4f12-a5be-6c4ae30ae1f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translate the text that is delimited by triple backticks into a style that is a polite tone that speaks in English Pirate.\n",
            "text: ```Hey there customer, the warranty does not cover cleaning expenses for your kitchen because it's your fault that you misused your blender by forgetting to put the lid on before starting the blender. Tough luck! See ya!\n",
            "```\n",
            "\n"
          ]
        }
      ],
      "source": [
        "service_messages = prompt_template.format_messages(\n",
        "    style=service_style_pirate,\n",
        "    text=service_reply)\n",
        "\n",
        "print(service_messages[0].content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0ae5552",
      "metadata": {
        "height": 47,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0ae5552",
        "outputId": "540a8ee2-b6c3-49f0-8f8c-320768b58728"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ahoy, me hearty! I be sorry to inform ye that the warranty be not coverin' the costs of cleanin' yer galley, as it seems ye may have misused yer blender by forgettin' to secure the lid afore settin' it to whirl. Aye, 'tis a bit of tough luck, indeed! Fair winds to ye, and may yer future blends be lid-secured!\n"
          ]
        }
      ],
      "source": [
        "service_response = chat.invoke(service_messages)\n",
        "print(service_response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36536e79",
      "metadata": {
        "id": "36536e79"
      },
      "source": [
        "## Output Parsers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start with defining how we would like the LLM output to look like:"
      ],
      "metadata": {
        "id": "gZ7N5iwTcPJD"
      },
      "id": "gZ7N5iwTcPJD"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1ccdff5",
      "metadata": {
        "height": 98,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1ccdff5",
        "outputId": "30f963da-eb56-492b-9c67-7540842e6856"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'gift': False, 'delivery_days': 5, 'price_value': 'pretty affordable!'}"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "{\n",
        "  \"gift\": False,\n",
        "  \"delivery_days\": 5,\n",
        "  \"price_value\": \"pretty affordable!\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df0f4680",
      "metadata": {
        "height": 540,
        "tags": [],
        "id": "df0f4680"
      },
      "outputs": [],
      "source": [
        "customer_review = \"\"\"\n",
        "This leaf blower is pretty amazing.  It has four settings: candle blower, gentle breeze, windy city, and tornado. It arrived in two days, just in time for my wife's anniversary present. I think my wife liked it so much she was speechless. So far I've been the only one using it, and I've been using it every other morning to clear the leaves on our lawn. It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\n",
        "\"\"\"\n",
        "\n",
        "review_template = \"\"\"\n",
        "For the following text, extract the following information:\n",
        "\n",
        "gift: Was the item purchased as a gift for someone else? Answer True if yes, False if not or unknown.\n",
        "\n",
        "delivery_days: How many days did it take for the product to arrive? If this information is not found, output -1.\n",
        "\n",
        "price_value: Extract any sentences about the value or price, and output them as a comma separated Python list.\n",
        "\n",
        "Format the output as JSON with the following keys:\n",
        "gift\n",
        "delivery_days\n",
        "price_value\n",
        "\n",
        "text: {text}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2386e9c",
      "metadata": {
        "height": 81,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2386e9c",
        "outputId": "22ae89ed-813b-46ec-d7e6-68fafe506d75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_variables=['text'] input_types={} partial_variables={} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['text'], input_types={}, partial_variables={}, template='\\nFor the following text, extract the following information:\\n\\ngift: Was the item purchased as a gift for someone else? Answer True if yes, False if not or unknown.\\n\\ndelivery_days: How many days did it take for the product to arrive? If this information is not found, output -1.\\n\\nprice_value: Extract any sentences about the value or price, and output them as a comma separated Python list.\\n\\nFormat the output as JSON with the following keys:\\ngift\\ndelivery_days\\nprice_value\\n\\ntext: {text}\\n'), additional_kwargs={})]\n"
          ]
        }
      ],
      "source": [
        "# create the second template\n",
        "prompt_template = ChatPromptTemplate.from_template(review_template)\n",
        "print(prompt_template) # notice that here we have  just one input variable(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "121bb0d4",
      "metadata": {
        "height": 81,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "121bb0d4",
        "outputId": "3521e94b-2d77-47c7-dfb2-70e6269d662f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```json\n",
            "{\n",
            "  \"gift\": true,\n",
            "  \"delivery_days\": 2,\n",
            "  \"price_value\": [\"It's slightly more expensive than the other leaf blowers out there\", \"I think it's worth it for the extra features\"]\n",
            "}\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "messages = prompt_template.format_messages(text=customer_review)\n",
        "chat = ChatOpenAI(temperature=0.0, model=llm_model)\n",
        "response = chat.invoke(messages)\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10de1d28",
      "metadata": {
        "height": 30,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10de1d28",
        "outputId": "277b3173-6537-4520-9631-12b0f7fa8639"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "str"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "# Inspect the type of the output (is it a dict?)\n",
        "type(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a3c0609",
      "metadata": {
        "height": 81,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "id": "3a3c0609",
        "outputId": "686a1400-3334-47cc-ab9a-8a00d240dd05"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'str' object has no attribute 'get'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2715072814.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# You will get an error by running this line of code because'gift' is not a dictionary 'gift' is a string\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'gift'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'get'"
          ]
        }
      ],
      "source": [
        "# You will get an error by running this line of code because'gift' is not a dictionary 'gift' is a string\n",
        "response.content.get('gift')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f7de2b8",
      "metadata": {
        "id": "5f7de2b8"
      },
      "source": [
        "### Parse the LLM output string into a Python dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2e0ec49",
      "metadata": {
        "height": 47,
        "tags": [],
        "id": "c2e0ec49"
      },
      "outputs": [],
      "source": [
        "from langchain.output_parsers.structured import ResponseSchema, StructuredOutputParser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9dea24b4",
      "metadata": {
        "height": 353,
        "tags": [],
        "id": "9dea24b4"
      },
      "outputs": [],
      "source": [
        "# 1. Define response schemas\n",
        "gift_schema = ResponseSchema(\n",
        "    name=\"gift\",\n",
        "    description=\"Was the item purchased as a gift for someone else? Answer True if yes, False if not or unknown.\")\n",
        "delivery_days_schema = ResponseSchema(\n",
        "    name=\"delivery_days\",\n",
        "    description=\"How many days did it take for the product to arrive? If this information is not found, output -1.\")\n",
        "price_value_schema = ResponseSchema(\n",
        "    name=\"price_value\",\n",
        "    description=\"Extract any sentences about the value or price, and output them as a comma separated Python list.\")\n",
        "\n",
        "response_schemas = [gift_schema, delivery_days_schema, price_value_schema]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b57e1ba8",
      "metadata": {
        "height": 45,
        "tags": [],
        "id": "b57e1ba8"
      },
      "outputs": [],
      "source": [
        "# 2. Create parser\n",
        "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`output_parser` is an object of **StructuredOutputParser** with several methods. Most common of them are:\n",
        "\n",
        "- **`get_format_instructions()`** – Returns a string of instructions for the LLM, telling it how to format its output (usually in JSON) so that `parse()` can extract the information correctly.\n",
        "\n",
        "- **`parse(text)`** – Takes the LLM’s output string and converts it into a Python dictionary (or structured object) following the response schemas you defined.\n",
        "\n",
        "- **`response_schemas`** – Shows the list of schemas you used when creating the parser, useful for reference or debugging.\n"
      ],
      "metadata": {
        "id": "vXAnPBA1rBXY"
      },
      "id": "vXAnPBA1rBXY"
    },
    {
      "cell_type": "code",
      "source": [
        "# List all methods and attributes\n",
        "print(dir(output_parser))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JMAf6L_rpCXo",
        "outputId": "9351e104-be37-4222-fadd-cc156c390c58"
      },
      "id": "JMAf6L_rpCXo",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['InputType', 'OutputType', '__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__or__', '__orig_bases__', '__parameters__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_on_complete__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_setattr_handlers__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__ror__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__weakref__', '_abatch_with_config', '_abc_impl', '_acall_with_config', '_atransform_stream_with_config', '_batch_with_config', '_calculate_keys', '_call_with_config', '_copy_and_set_values', '_get_value', '_iter', '_setattr_handler', '_transform_stream_with_config', '_type', 'abatch', 'abatch_as_completed', 'ainvoke', 'aparse', 'aparse_result', 'as_tool', 'assign', 'astream', 'astream_events', 'astream_log', 'atransform', 'batch', 'batch_as_completed', 'bind', 'config_schema', 'config_specs', 'configurable_alternatives', 'configurable_fields', 'construct', 'copy', 'dict', 'from_orm', 'from_response_schemas', 'get_config_jsonschema', 'get_format_instructions', 'get_graph', 'get_input_jsonschema', 'get_input_schema', 'get_lc_namespace', 'get_name', 'get_output_jsonschema', 'get_output_schema', 'get_prompts', 'input_schema', 'invoke', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'map', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'name', 'output_schema', 'parse', 'parse_file', 'parse_obj', 'parse_raw', 'parse_result', 'parse_with_prompt', 'pick', 'pipe', 'response_schemas', 'schema', 'schema_json', 'stream', 'to_json', 'to_json_not_implemented', 'transform', 'update_forward_refs', 'validate', 'with_alisteners', 'with_config', 'with_fallbacks', 'with_listeners', 'with_retry', 'with_types']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# help(output_parser)"
      ],
      "metadata": {
        "id": "P8YTt4noqwld"
      },
      "id": "P8YTt4noqwld",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fdeaf4fc",
      "metadata": {
        "height": 30,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdeaf4fc",
        "outputId": "e63af269-e1d7-4005-ef8c-efd1e16fa0c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
            "\n",
            "```json\n",
            "{\n",
            "\t\"gift\": string  // Was the item purchased as a gift for someone else? Answer True if yes, False if not or unknown.\n",
            "\t\"delivery_days\": string  // How many days did it take for the product to arrive? If this information is not found, output -1.\n",
            "\t\"price_value\": string  // Extract any sentences about the value or price, and output them as a comma separated Python list.\n",
            "}\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "# 3. Get LLM format instructions\n",
        "format_instructions = output_parser.get_format_instructions()\n",
        "print(format_instructions)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "parser_schemas = output_parser.response_schemas\n",
        "print(parser_schemas)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82iHc6u8riyl",
        "outputId": "3ddb4c16-6b65-42e8-c830-43207d85b0ab"
      },
      "id": "82iHc6u8riyl",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ResponseSchema(name='gift', description='Was the item purchased as a gift for someone else? Answer True if yes, False if not or unknown.', type='string'), ResponseSchema(name='delivery_days', description='How many days did it take for the product to arrive? If this information is not found, output -1.', type='string'), ResponseSchema(name='price_value', description='Extract any sentences about the value or price, and output them as a comma separated Python list.', type='string')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "082947fc",
      "metadata": {
        "height": 385,
        "tags": [],
        "id": "082947fc"
      },
      "outputs": [],
      "source": [
        "# 4. Example\n",
        "review_template_2 = \"\"\"For the following text, extract the following information:\n",
        "\n",
        "gift: Was the item purchased as a gift for someone else? Answer True if yes, False if not or unknown.\n",
        "\n",
        "delivery_days: How many days did it take for the product to arrive? If this information is not found, output -1.\n",
        "\n",
        "price_value: Extract any sentences about the value or price, and output them as a comma separated Python list.\n",
        "\n",
        "text: {text}\n",
        "\n",
        "{format_instructions}\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template=review_template_2)\n",
        "\n",
        "messages = prompt.format_messages(text=customer_review, format_instructions=format_instructions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f1537a7",
      "metadata": {
        "height": 30,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1f1537a7",
        "outputId": "76d672ed-69e9-4461-e567-0aae50cb26e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For the following text, extract the following information:\n",
            "\n",
            "gift: Was the item purchased as a gift for someone else? Answer True if yes, False if not or unknown.\n",
            "\n",
            "delivery_days: How many days did it take for the product to arrive? If this information is not found, output -1.\n",
            "\n",
            "price_value: Extract any sentences about the value or price, and output them as a comma separated Python list.\n",
            "\n",
            "text: \n",
            "This leaf blower is pretty amazing.  It has four settings: candle blower, gentle breeze, windy city, and tornado. It arrived in two days, just in time for my wife's anniversary present. I think my wife liked it so much she was speechless. So far I've been the only one using it, and I've been using it every other morning to clear the leaves on our lawn. It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\n",
            "\n",
            "\n",
            "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
            "\n",
            "```json\n",
            "{\n",
            "\t\"gift\": string  // Was the item purchased as a gift for someone else? Answer True if yes, False if not or unknown.\n",
            "\t\"delivery_days\": string  // How many days did it take for the product to arrive? If this information is not found, output -1.\n",
            "\t\"price_value\": string  // Extract any sentences about the value or price, and output them as a comma separated Python list.\n",
            "}\n",
            "```\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(messages[0].content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b663657",
      "metadata": {
        "height": 30,
        "tags": [],
        "id": "7b663657"
      },
      "outputs": [],
      "source": [
        "response = chat.invoke(messages)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g8qFlhyKp1G3",
        "outputId": "86ca4889-406e-4641-f433-405a44eb5f91"
      },
      "id": "g8qFlhyKp1G3",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='```json\\n{\\n\\t\"gift\": \"True\",\\n\\t\"delivery_days\": \"2\",\\n\\t\"price_value\": \"It\\'s slightly more expensive than the other leaf blowers out there, but I think it\\'s worth it for the extra features.\"\\n}\\n```', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 51, 'prompt_tokens': 316, 'total_tokens': 367, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-Cb6vpUrMirHncGT2dYAgwEy6QjjbL', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--78e66b52-2857-47b9-8b7e-bee8a746e1a0-0', usage_metadata={'input_tokens': 316, 'output_tokens': 51, 'total_tokens': 367, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8c3a9fe",
      "metadata": {
        "height": 30,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8c3a9fe",
        "outputId": "98212155-84c9-4cea-a5f3-ac767c16a8aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```json\n",
            "{\n",
            "\t\"gift\": \"True\",\n",
            "\t\"delivery_days\": \"2\",\n",
            "\t\"price_value\": \"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\"\n",
            "}\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "print(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "904f1c25",
      "metadata": {
        "height": 30,
        "tags": [],
        "id": "904f1c25"
      },
      "outputs": [],
      "source": [
        "output_dict = output_parser.parse(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d48b647a",
      "metadata": {
        "height": 30,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d48b647a",
        "outputId": "ce6abe08-0a0e-4932-b159-48d8ff2c58d6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'gift': 'True',\n",
              " 'delivery_days': '2',\n",
              " 'price_value': \"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "output_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4346150f",
      "metadata": {
        "height": 30,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4346150f",
        "outputId": "df94711c-da03-41bb-a382-378ec9a9d28f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ],
      "source": [
        "type(output_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a833fcea",
      "metadata": {
        "height": 30,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "a833fcea",
        "outputId": "eea60c74-69b8-4533-f037-130623674f11"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 48
        }
      ],
      "source": [
        "output_dict.get('delivery_days')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, we explored how to work with **LangChain templates and structured outputs**. Templates allow us to standardize instructions to the LLM, specifying input variables like text and style, so we don't repeat general instructions each time. We can also define structured output formats directly in the prompt, instructing the LLM to return JSON with keys such as gift, delivery_days, and price_value. However, for greater reliability and convenience, LangChain provides structured parsing using tools like ```ResponseSchema``` and ```StructuredOutputParser``` (or Pydantic models in modern versions). These parsers **validate the output, enforce required fields, and automatically convert responses into Python dictionaries**, ensuring consistency and making downstream use of the data straightforward. Together, templates and structured parsing form a robust workflow for building reliable LLM applications."
      ],
      "metadata": {
        "id": "FtzgBd8ytWmF"
      },
      "id": "FtzgBd8ytWmF"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38d6a0f7",
      "metadata": {
        "height": 30,
        "id": "38d6a0f7"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}