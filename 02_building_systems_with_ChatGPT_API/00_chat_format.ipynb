{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GiX007/agent-labs/blob/main/02_building_systems_with_ChatGPT_API/00_chat_format.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae5bcee9-6588-4d29-bbb9-6fb351ef6630",
      "metadata": {
        "id": "ae5bcee9-6588-4d29-bbb9-6fb351ef6630"
      },
      "source": [
        "# Chat Format and Tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c797991-8486-4d79-8c1d-5dc0c1289c2f",
      "metadata": {
        "id": "0c797991-8486-4d79-8c1d-5dc0c1289c2f"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Load the API key and relevant Python libaries."
      ],
      "metadata": {
        "id": "T7BaU2LEAURT"
      },
      "id": "T7BaU2LEAURT"
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we load the OpenAI API key (like we did in the previous course)."
      ],
      "metadata": {
        "id": "B9mPuwzLAWUc"
      },
      "id": "B9mPuwzLAWUc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19cd4e96",
      "metadata": {
        "height": 132,
        "id": "19cd4e96"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "import tiktoken\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "dotenv_path = find_dotenv() or '/content/OPENAI_API_KEY.env' # read local .env file\n",
        "load_dotenv(dotenv_path)\n",
        "\n",
        "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
        "client = openai.OpenAI(api_key=openai_api_key)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47ba0938-7ca5-46c4-a9d1-b55708d4dc7c",
      "metadata": {
        "id": "47ba0938-7ca5-46c4-a9d1-b55708d4dc7c"
      },
      "source": [
        "#### helper function\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This may look familiar if you took the earlier course \"ChatGPT Prompt Engineering for Developers\" Course.\n",
        "\n",
        "Throughout this course, we will use OpenAI's `gpt-4o-mini` model and the [chat completions endpoint](https://platform.openai.com/docs/guides/chat).\n",
        "\n",
        "This helper function will make it easier to use prompts and look at the generated outputs."
      ],
      "metadata": {
        "id": "keVBIC3WB_qH"
      },
      "id": "keVBIC3WB_qH"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ed96988",
      "metadata": {
        "height": 164,
        "id": "1ed96988"
      },
      "outputs": [],
      "source": [
        "def get_completion(prompt, model=\"gpt-4o-mini\"):\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=0, # this is the degree of randomness of the model's output\n",
        "    )\n",
        "    return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe10a390-2461-447d-bf8b-8498db404c44",
      "metadata": {
        "id": "fe10a390-2461-447d-bf8b-8498db404c44"
      },
      "source": [
        "## Prompt the model and get a completion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1cc57b2",
      "metadata": {
        "height": 30,
        "id": "e1cc57b2"
      },
      "outputs": [],
      "source": [
        "response = get_completion(\"What is the capital of France?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76774108",
      "metadata": {
        "height": 30,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76774108",
        "outputId": "38beedb3-250f-457d-f964-c2a04cf26587"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The capital of France is Paris.\n"
          ]
        }
      ],
      "source": [
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b83d4e38-3e3c-4c5a-a949-040a27f29d63",
      "metadata": {
        "id": "b83d4e38-3e3c-4c5a-a949-040a27f29d63"
      },
      "source": [
        "## Tokens"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = get_completion(\"Take the letters in lollipop and reverse them\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I11WQNnNCxEJ",
        "outputId": "35362aad-a23a-46e5-bcb1-4393a1f5bfa2"
      },
      "id": "I11WQNnNCxEJ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reversing the letters in \"lollipop\" gives you \"popillol.\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following example is coming from ann older implementation using the model ```gpt-3.5-turbo```. Notice the how the reply differs and how we got the correct answer."
      ],
      "metadata": {
        "id": "51g2Oy7ZC9dg"
      },
      "id": "51g2Oy7ZC9dg"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc2d9e40",
      "metadata": {
        "height": 64,
        "id": "cc2d9e40",
        "outputId": "6e9cbaf6-62e7-415a-db25-f07870918cfa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pilpolol\n"
          ]
        }
      ],
      "source": [
        "response = get_completion(\"Take the letters in lollipop and reverse them\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d2b14d0-749d-4a79-9812-7b00ace9ae6f",
      "metadata": {
        "id": "9d2b14d0-749d-4a79-9812-7b00ace9ae6f"
      },
      "source": [
        "\"lollipop\" in reverse should be \"popillol\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37cab84f",
      "metadata": {
        "height": 47,
        "id": "37cab84f"
      },
      "outputs": [],
      "source": [
        "response = get_completion(\"\"\"Take the letters in l-o-l-l-i-p-o-p and reverse them\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1577c561",
      "metadata": {
        "height": 30,
        "id": "1577c561",
        "outputId": "90883719-b264-4273-b3d4-25cb0fd9b657"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'p-o-p-i-l-l-o-l'"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8b88940-d3ab-4c00-b5c0-31531deaacbd",
      "metadata": {
        "id": "c8b88940-d3ab-4c00-b5c0-31531deaacbd"
      },
      "source": [
        "## Helper function (chat format)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f89efad",
      "metadata": {
        "height": 215,
        "id": "8f89efad"
      },
      "outputs": [],
      "source": [
        "def get_completion_from_messages(messages,\n",
        "                                 model=\"gpt-4o-mini\",\n",
        "                                 temperature=0,\n",
        "                                 max_tokens=500):\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=temperature, # this is the degree of randomness of the model's output\n",
        "        max_tokens=max_tokens, # the maximum number of tokens the model can ouptut\n",
        "    )\n",
        "    return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Effect of Temperature on LLM Output\n",
        "\n",
        "Temperature is a crucial parameter that controls the degree of randomness and creativity in the model's output. For engineering robust applications, this setting must be carefully chosen.\n",
        "\n",
        "| Temperature Value | Randomness/Creativity | Effect on Output | Best Use Case |\n",
        "| :--- | :--- | :--- | :--- |\n",
        "| **0.0** (or very close to zero) | **Minimal** (Deterministic) | The model always selects the most statistically probable next token. Outputs are factual, concise, and repeatable. | **Factual Q&A, Summarization, Code Generation, Classification, and Agentic Workflows.** |\n",
        "| **1.0** (or higher) | **Maximal** (High Variability) | The model considers many less-likely tokens, leading to more \"surprising\" or diverse outputs. The results are less predictable and may occasionally hallucinate. | **Brainstorming, Creative Writing, Story Generation, Ideation.** |"
      ],
      "metadata": {
        "id": "Y--Fj9_bEsTj"
      },
      "id": "Y--Fj9_bEsTj"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b28c3424",
      "metadata": {
        "height": 198,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b28c3424",
        "outputId": "9d5cbace-2cb1-4780-a73b-e209cc6e3f66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In the garden, bright and fair,  \n",
            "Lives a jolly, jolly carrot, with a top of orange hair.  \n",
            "He giggles with glee, in the sun’s warm glow,  \n",
            "Wiggling his leaves, as he sways to and fro.  \n",
            "\n",
            "“I'm crunchy, I'm sweet, oh what a delight!  \n",
            "A nibble from me makes the veggies feel bright!”  \n",
            "So if you find a carrot, so happy and spry,  \n",
            "Give him a hug, let your spirit fly high!\n"
          ]
        }
      ],
      "source": [
        "messages =  [\n",
        "{'role':'system', # specify the overall tone of llm\n",
        " 'content':\"\"\"You are an assistant who responds in the style of Dr Seuss.\"\"\"},\n",
        "{'role':'user', # specific instruction that llm will carry out\n",
        " 'content':\"\"\"write me a very short poem about a happy carrot\"\"\"},\n",
        "]\n",
        "response = get_completion_from_messages(messages, temperature=1)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56c6978d",
      "metadata": {
        "height": 198,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56c6978d",
        "outputId": "fd68803f-a407-4f0f-d56a-5b7413999e11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Once upon a time in a vibrant garden, a happy carrot named Carl danced under the warm sun, dreaming of becoming a delicious salad.\n"
          ]
        }
      ],
      "source": [
        "# length\n",
        "messages =  [\n",
        "{'role':'system',\n",
        " 'content':'All your responses must be one sentence long.'},\n",
        "{'role':'user',\n",
        " 'content':'write me a story about a happy carrot'},\n",
        "]\n",
        "response = get_completion_from_messages(messages, temperature =1)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14fd6331",
      "metadata": {
        "height": 217,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14fd6331",
        "outputId": "59022f76-3d14-405a-a48d-2b40c615c1d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In a garden so bright, with sunshine and cheer, lived a carrot named Carl who spread joy far and near!\n"
          ]
        }
      ],
      "source": [
        "# combined (style + length)\n",
        "messages =  [\n",
        "{'role':'system',\n",
        " 'content':\"\"\"You are an assistant who responds in the style of Dr Seuss. All your responses must be one sentence long.\"\"\"},\n",
        "{'role':'user',\n",
        " 'content':\"\"\"write me a story about a happy carrot\"\"\"},\n",
        "]\n",
        "response = get_completion_from_messages(messages, temperature =1)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89a70c79",
      "metadata": {
        "height": 387,
        "id": "89a70c79"
      },
      "outputs": [],
      "source": [
        "def get_completion_and_token_count(messages,\n",
        "                                   model=\"gpt-4o-mini\",\n",
        "                                   temperature=0,\n",
        "                                   max_tokens=500):\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=temperature,\n",
        "        max_tokens=max_tokens,\n",
        "    )\n",
        "    # print(response) # returns a dict\n",
        "\n",
        "    content = response.choices[0].message.content\n",
        "\n",
        "    token_dict = {\n",
        "        'prompt_tokens':response.usage.prompt_tokens,\n",
        "        'completion_tokens':response.usage.completion_tokens,\n",
        "        'total_tokens':response.usage.total_tokens,\n",
        "    }\n",
        "\n",
        "    return content, token_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a64cf3c6",
      "metadata": {
        "height": 181,
        "id": "a64cf3c6"
      },
      "outputs": [],
      "source": [
        "messages = [\n",
        "{'role':'system',\n",
        " 'content':\"\"\"You are an assistant who responds in the style of Dr Seuss.\"\"\"},\n",
        "{'role':'user',\n",
        " 'content':\"\"\"write me a very short poem about a happy carrot\"\"\"},\n",
        "]\n",
        "response, token_dict = get_completion_and_token_count(messages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cfd8fbd4",
      "metadata": {
        "height": 30,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cfd8fbd4",
        "outputId": "3cadfe4a-6f52-4c24-a32e-a2f0bc17e260"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In a garden so bright, where the sun shines so fair,  \n",
            "Lived a jolly young carrot with bright orange hair.  \n",
            "He danced with the daisies, he twirled with the bees,  \n",
            "“Oh, life is delightful!” he sang in the breeze.  \n",
            "\n",
            "With a wiggle and giggle, he’d wiggle all day,  \n",
            "In the soil so warm, he would laugh and would play.  \n",
            "For a happy young carrot, with joy in his heart,  \n",
            "Knows that life in the garden is truly an art!\n"
          ]
        }
      ],
      "source": [
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "352ad320",
      "metadata": {
        "height": 30,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "352ad320",
        "outputId": "885279bb-01c6-4809-f230-5e2ccfdcf486"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'prompt_tokens': 35, 'completion_tokens': 111, 'total_tokens': 146}\n"
          ]
        }
      ],
      "source": [
        "print(token_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65372cdd-d869-4768-947a-0173e7f96335",
      "metadata": {
        "id": "65372cdd-d869-4768-947a-0173e7f96335"
      },
      "source": [
        "#### Notes on using the OpenAI API outside of this classroom\n",
        "\n",
        "To install the OpenAI Python library:\n",
        "```\n",
        "!pip install openai\n",
        "```\n",
        "\n",
        "The library needs to be configured with your account's secret key, which is available on the [website](https://platform.openai.com/account/api-keys).\n",
        "\n",
        "You can either set it as the `OPENAI_API_KEY` environment variable before using the library:\n",
        " ```\n",
        " !export OPENAI_API_KEY='sk-...'\n",
        " ```\n",
        "\n",
        "Or, set `openai.api_key` to its value (if you only run locally):\n",
        "\n",
        "```\n",
        "import openai\n",
        "openai.api_key = \"sk-...\"\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "294dcc54-f941-422e-8602-d8e78a0da093",
      "metadata": {
        "height": 30,
        "id": "294dcc54-f941-422e-8602-d8e78a0da093"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}